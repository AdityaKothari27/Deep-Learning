{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Name - Aditya Kothari\n",
        "\n",
        "Roll no. - I028\n",
        "\n",
        "Batch  - B1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "R8ziSfGAKE5x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARxH6ZokKcaV"
      },
      "source": [
        "Task 1:\n",
        "\n",
        " Perform basic operations using Tensorflow (Defining constants, variables, concatenation, add, multiply, reduce mean, reduce sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Q-OA3ciKy97",
        "outputId": "0f24e95b-c180-4177-d86e-7ff6c5945a8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 1  2  3  4  5  6  7  8]\n",
            " [ 5  6  7  8  9 10 11 12]]\n",
            "\n",
            "On Adding:\n",
            "\n",
            "[[ 6  8 10 12]\n",
            " [14 16 18 20]]\n",
            "\n",
            "On Multiplying:\n",
            "\n",
            "[[ 5 12 21 32]\n",
            " [45 60 77 96]]\n"
          ]
        }
      ],
      "source": [
        "#Defining constants and variables and concatinating them\n",
        "A = tf.constant([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
        "B = tf.Variable([[5, 6, 7, 8], [9, 10, 11, 12]])\n",
        "\n",
        "AB_concat = tf.concat([A, B], axis=1)\n",
        "print(AB_concat.numpy())\n",
        "\n",
        "\n",
        "#Adding, multiplying\n",
        "print(\"\\nOn Adding:\\n\")\n",
        "AB_add = tf.add(A, B)\n",
        "print(AB_add.numpy())\n",
        "\n",
        "print(\"\\nOn Multiplying:\\n\")\n",
        "AB_multiply = tf.multiply(A, B)\n",
        "print(AB_multiply.numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iNl9yNULZ1Y",
        "outputId": "b8465b22-301d-496d-b940-585645e3f0b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        }
      ],
      "source": [
        "#Reduce Mean\n",
        "AB_reduce_mean = tf.reduce_mean(A)\n",
        "print(AB_reduce_mean.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR7ENF5vNokb",
        "outputId": "ed4080b2-482e-47fa-8bb5-5e454d59c983"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3 4 5 6]\n"
          ]
        }
      ],
      "source": [
        "#Reduce Mean with axis = 0\n",
        "AB_reduce_mean = tf.reduce_mean(A, 0)\n",
        "print(AB_reduce_mean.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wBu4RbpMDG8",
        "outputId": "fe23fd97-a991-4231-ac52-09a0ba4d13f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36\n"
          ]
        }
      ],
      "source": [
        "#Reduce sum\n",
        "AB_reduce_mean = tf.reduce_sum(A)\n",
        "print(AB_reduce_mean.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aioWxqggNtMV",
        "outputId": "c49521ea-abf8-44b2-d7ef-7e4cffc49930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 6  8 10 12]\n"
          ]
        }
      ],
      "source": [
        "#Reduce sum with axis = 0\n",
        "AB_reduce_mean = tf.reduce_sum(A, 0)\n",
        "print(AB_reduce_mean.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njeKFcGNNgo1"
      },
      "source": [
        "Task 2:\n",
        "\n",
        "Perform linear algebra operations using Tensorflow (transpose, matrix multiplication, elementwise multiplication, determinant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sExdhTTxOpB8",
        "outputId": "ee7302af-0b62-4681-e9c4-288e72f23b03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1 5]\n",
            " [2 6]\n",
            " [3 7]\n",
            " [4 8]]\n"
          ]
        }
      ],
      "source": [
        "#Transposing matrix A\n",
        "A_transpose = tf.transpose(A)\n",
        "print(A_transpose.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy4d-4GlO9oS",
        "outputId": "0c8f3953-e01f-4ba7-b32a-35b011bcffe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 5600]\n",
            " [ 5900]\n",
            " [10550]\n",
            " [ 6550]]\n"
          ]
        }
      ],
      "source": [
        "#Matrix multiplication\n",
        "features = tf.constant([[2, 24], [2, 26], [2, 57], [1, 37]])\n",
        "params = tf.constant([[1000], [150]])\n",
        "AB_multiplication = tf.linalg.matmul(features, params)\n",
        "print(AB_multiplication.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLGgSkpXNjCQ",
        "outputId": "a6aee118-47cd-48d7-8daa-43c5cb23915d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "C1: [1 2 3 4]\n",
            "C23: [[1 2 3]\n",
            " [1 6 4]]\n"
          ]
        }
      ],
      "source": [
        "A1 = tf.constant([1, 2, 3, 4])\n",
        "A23 = tf.constant([[1, 2, 3], [1, 6, 4]])\n",
        "\n",
        "#Define B1 and B23 to have the correct shape\n",
        "B1 = tf.ones_like(A1)\n",
        "B23 = tf.ones_like(A23)\n",
        "\n",
        "#Perform element-wise multiplication\n",
        "C1 = tf.multiply(A1, B1)\n",
        "C23 = tf.multiply(A23, B23)\n",
        "\n",
        "#Print the tensors C1 and C23\n",
        "print('C1: {}'.format(C1.numpy()))\n",
        "print('C23: {}'.format(C23.numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqgEb_zkOZT4",
        "outputId": "b43e81e4-78c6-4525-c504-43f19879645f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 2 24]\n",
            " [ 2 26]\n",
            " [ 2 57]\n",
            " [ 1 37]]\n"
          ]
        }
      ],
      "source": [
        "#Element wise multiplication\n",
        "B1 = tf.ones_like(features)\n",
        "AB_element_wise = tf.multiply(features, B1)\n",
        "print(AB_element_wise.numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0LrfTYFQDq4",
        "outputId": "f7204b04-f447-4bc4-c41d-7a8a9cb90cbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-4.0\n"
          ]
        }
      ],
      "source": [
        "#Determinant\n",
        "A = tf.constant([[1, 2], [5, 6]], dtype=tf.float32)\n",
        "A_determinant = tf.cast(A, dtype=tf.float32)\n",
        "A_determinant = tf.linalg.det(A)\n",
        "print(A_determinant.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghC2ndRxRe8s"
      },
      "source": [
        "Task 3:\n",
        "\n",
        "Perform derivative and higher order derivative for function f(x) = x^3 using gradient tape of Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzFMj3ujQZlD",
        "outputId": "15025e48-2371-4333-e9d3-de653f317ea9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
            "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First order derivative:  48.0\n",
            "Second order derivative:  24.0\n"
          ]
        }
      ],
      "source": [
        "#Perform derivative and higher order derivative for function f(x) = x^3 using gradient tape of Tensorflow\n",
        "x = tf.Variable(4.0)\n",
        "\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  y = x**3\n",
        "  dy_dx = tape.gradient(y, x)\n",
        "  dy_dx_2 = tape.gradient(dy_dx, x)\n",
        "print(\"First order derivative: \", dy_dx.numpy())\n",
        "print(\"Second order derivative: \", dy_dx_2.numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R72CF7E_TVuW"
      },
      "source": [
        "Task 4:\n",
        "\n",
        "Compute WX+b using Tensorflow where W, X,  and b are drawn from a random normal distribution. W is of shape (4, 3), X is (3,1) and b is (4,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ1UDM43TAd2",
        "outputId": "f9689e88-d50b-4fe5-e23c-264869e61001"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-1.1527684]\n",
            " [-2.1059577]\n",
            " [-0.3005672]\n",
            " [-0.7499763]], shape=(4, 1), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "W = tf.Variable(tf.random.normal((4, 3)))\n",
        "X = tf.Variable(tf.random.normal((3, 1)))\n",
        "B = tf.Variable(tf.random.normal((4, 1)))\n",
        "\n",
        "print((tf.matmul(W, X) + B.numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXL44PiqVFJx"
      },
      "source": [
        "Task 5:\n",
        "\n",
        "Compute Gradient of sigmoid function using Tensor flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-rlvlzJThQs",
        "outputId": "1aada39e-a0b7-4221-b95d-d5c525d9097c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.25\n"
          ]
        }
      ],
      "source": [
        "x = tf.constant(0.0)\n",
        "with tf.GradientTape(persistent=True) as tape:\n",
        "  tape.watch(x)\n",
        "  y = tf.nn.sigmoid(x)\n",
        "\n",
        "grad = tape.gradient(y, x) # dy/dx\n",
        "print(grad.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fu2FJpaeVxDW"
      },
      "source": [
        "Task 6:\n",
        "\n",
        "Identify two research papers based on deep learning. State for which application they have used deep learning. (Cite the papers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2Db8QGZWr9E"
      },
      "source": [
        "1. **ImageNet Classification with Deep Convolutional Neural Networks**\n",
        "\n",
        "**Authors:** Alex Krizhevsky, Ilya Sutskever, Geoffrey E. Hinton\n",
        "\n",
        "**Application:** This paper introduced the concept of using deep convolutional neural networks (CNNs) for image classification tasks. It proposed the architecture known as \"AlexNet,\" which achieved a breakthrough in the ImageNet Large Scale Visual Recognition Challenge in 2012, significantly advancing the field of image classification using deep learning.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "2. **Attention Is All You Need**\n",
        "\n",
        "**Authors:** Ashish Vaswani, Noam Shazeer, Niki Parmar, et al.\n",
        "\n",
        "**Application:** This paper introduced the Transformer architecture, which leverages self-attention mechanisms to process sequences in parallel, making it highly effective for tasks involving sequential data, such as machine translation, text generation, and language understanding. The Transformer architecture marked a significant shift in natural language processing and has also found applications in other domains.\n",
        "Paper: Attention Is All You Need"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
